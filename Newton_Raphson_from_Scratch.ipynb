{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Softmax function for vector x\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "def train_softmax_regression(X, y, num_classes, max_iters=100, tol=1e-5):\n",
        "    \"\"\"\n",
        "    Trains a softmax regression model using the Newton-Raphson method.\n",
        "\n",
        "    Parameters:\n",
        "    - X: feature matrix of shape (num_data, num_features)\n",
        "    - y: target class vector of shape (num_data,)\n",
        "    - num_classes: number of classes\n",
        "    - max_iters: maximum number of iterations (default: 100)\n",
        "    - tol: convergence tolerance (default: 1e-5)\n",
        "\n",
        "    Returns:\n",
        "    - W: learned weights of shape (num_classes, num_features)\n",
        "    - b: learned biases of shape (num_classes,)\n",
        "    \"\"\"\n",
        "\n",
        "    # One-hot encode target classes\n",
        "    y_onehot = np.eye(num_classes)[y]\n",
        "\n",
        "    # Initialize weights and biases to zero\n",
        "    num_data, num_features = X.shape\n",
        "    W = np.zeros((num_classes, num_features))\n",
        "    b = np.zeros(num_classes)\n",
        "\n",
        "    # Train model using Newton-Raphson method\n",
        "    for i in range(max_iters):\n",
        "\n",
        "        # Compute probabilities using softmax function\n",
        "        logits = np.dot(X, W.T) + b\n",
        "        probs = np.apply_along_axis(softmax, 1, logits)\n",
        "\n",
        "        # Compute gradient and Hessian of negative log-likelihood\n",
        "        grad_J_W = 1/num_data * np.dot((probs - y_onehot).T, X)\n",
        "        grad_J_b = 1/num_data * np.sum(probs - y_onehot, axis=0)\n",
        "\n",
        "        Hessian_J_W = 1/num_data * np.zeros((num_features, num_classes, num_features))\n",
        "        Hessian_J_b = 1/num_data * np.zeros((num_classes, num_classes))\n",
        "\n",
        "        for j in range(num_classes):\n",
        "            probs_j = probs[:, j]\n",
        "            Hessian_J_W[j] = np.dot((probs_j * (1 - probs_j) * X.T), X)\n",
        "            Hessian_J_b[j, j] = np.sum(probs_j * (1 - probs_j))\n",
        "\n",
        "        # Apply Newton-Raphson update\n",
        "        W_new = W - np.linalg.inv(Hessian_J_W.sum(axis=0)).dot(grad_J_W.T)\n",
        "        b_new = b - np.linalg.inv(Hessian_J_b).dot(grad_J_b)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.max(np.abs(W_new - W)) < tol and np.max(np.abs(b_new - b)) < tol:\n",
        "            W = W_new\n",
        "            b = b_new\n",
        "            break\n",
        "\n",
        "        # Update weights and biases\n",
        "        W = W_new\n",
        "        b = b_new\n",
        "\n",
        "    return W, b\n",
        "\n",
        "# # Generate some random data\n",
        "# X = np.random.randn(3, 4)\n",
        "# y = np.array([0, 1, 2])\n",
        "\n",
        "# # Train softmax regression model\n",
        "# num_classes = len(np.unique(y))\n",
        "# W, b = train_softmax_regression(X, y, num_classes)\n"
      ],
      "metadata": {
        "id": "eUYgLU3bJMwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate random data\n",
        "np.random.seed(123)\n",
        "X = np.random.randn(100, 5)\n",
        "y = np.random.randint(5, size=100)\n",
        "\n",
        "# Train softmax regression model\n",
        "W, b = train_softmax_regression(X, y, num_classes=5)\n",
        "\n",
        "# Print learned weights and biases\n",
        "print(\"Weights:\\n\", W)\n",
        "print(\"Biases:\\n\", b)\n",
        "X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ8Un0rMJ0sr",
        "outputId": "9d882348-2f3f-4923-a3df-55ea54530e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights:\n",
            " [[-0.02598701 -0.0365298   0.07210556  0.00702519 -0.01661395]\n",
            " [ 0.04208739 -0.07017528 -0.04297361  0.00232457  0.06873693]\n",
            " [-0.02376718  0.00071733  0.0726625  -0.08490233  0.03528968]\n",
            " [-0.0428995   0.00247003 -0.01639844  0.00863375  0.04819415]\n",
            " [ 0.03518724  0.01073504 -0.0116562   0.00514174 -0.03940783]]\n",
            "Biases:\n",
            " [ 0.13609204 -0.23515828 -0.1120175   0.13529996  0.0365125 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObdsJXofMAQL",
        "outputId": "45ecebdc-c37b-4289-be7a-261c7a0ea108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W, b = train_softmax_regression(X_train, y_train, 4)\n",
        "probs = np.apply_along_axis(softmax, 1, np.dot(X_test, W.T) + b)\n",
        "y_pred = np.argmax(probs, axis=1)\n",
        "acc = np.mean(y_pred == y_test)\n",
        "print(\"Accuracy: {:.2f}%\".format(acc * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y210bEgJORFm",
        "outputId": "07fcbcae-0a51-46cd-d592-85b6596a8bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 63.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Work in Progress"
      ],
      "metadata": {
        "id": "GO3wKSqX5ae7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the softmax function\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "# Define the gradient of the softmax function\n",
        "def softmax_grad(x):\n",
        "    p = softmax(x)\n",
        "    return p * (1 - p)\n",
        "\n",
        "# Define the Newton-Raphson update rule\n",
        "def newton_update(X, y, w):\n",
        "    p = softmax(X @ w)\n",
        "    grad = X.T @ (p - y)\n",
        "    hessian = X.T @ np.diag(p*(1-p)) @ X\n",
        "    w_new = w - np.linalg.inv(hessian) @ grad\n",
        "    return w_new\n",
        "\n",
        "# Load the Iris dataset and standardize the features\n",
        "iris = load_iris()\n",
        "X = StandardScaler().fit_transform(iris.data)\n",
        "y = np.eye(3)[iris.target]\n",
        "\n",
        "# Initialize the weights\n",
        "n_features = X.shape[1]\n",
        "n_classes = y.shape[1]\n",
        "w = np.zeros((n_features, n_classes))\n",
        "\n",
        "# Train the model using the Newton-Raphson method\n",
        "for i in range(50):\n",
        "    w = newton_update(X, y, w)\n",
        "\n",
        "# Make predictions and compute the accuracy\n",
        "p = softmax(X @ w)\n",
        "predictions = np.argmax(p, axis=1)\n",
        "accuracy = np.mean(predictions == iris.target)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "s2v9FhghOeAP",
        "outputId": "1d525754-cf80-485b-a491-1177ac87a5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d67ac7fcc29f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model using the Newton-Raphson method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Make predictions and compute the accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-d67ac7fcc29f>\u001b[0m in \u001b[0;36mnewton_update\u001b[0;34m(X, y, w)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 150)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(theta, x):\n",
        "    \"\"\"Softmax function for a sample x with weight vector theta.\"\"\"\n",
        "    e = np.exp(theta.dot(x))\n",
        "    return e / e.sum()\n",
        "\n",
        "def neg_log_likelihood(theta, X, y):\n",
        "    \"\"\"Negative log-likelihood function for weight vector theta.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    k = theta.shape[0] // X.shape[1]\n",
        "    J = 0\n",
        "    for i in range(n):\n",
        "        for j in range(k):\n",
        "            J -= y[i,j] * np.log(softmax(theta[j*X.shape[1]:(j+1)*X.shape[1]], X[i]))\n",
        "    return J / n\n",
        "\n",
        "def gradient(theta, X, y):\n",
        "    \"\"\"Gradient of the negative log-likelihood function for weight vector theta.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    k = theta.shape[0] // X.shape[1]\n",
        "    grad = np.zeros_like(theta)\n",
        "    for i in range(n):\n",
        "        for j in range(k):\n",
        "            grad[j*X.shape[1]:(j+1)*X.shape[1]] -= X[i] * (y[i,j] - softmax(theta[j*X.shape[1]:(j+1)*X.shape[1]], X[i]))\n",
        "    return grad / n\n",
        "\n",
        "def hessian(theta, X, y):\n",
        "    \"\"\"Hessian matrix of the negative log-likelihood function for weight vector theta.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    k = theta.shape[0] // X.shape[1]\n",
        "    hess = np.zeros((theta.shape[0], theta.shape[0]))\n",
        "    for i in range(n):\n",
        "        s = softmax(theta[0:X.shape[1]], X[i])\n",
        "        hess[0:X.shape[1], 0:X.shape[1]] += np.outer(s, s)\n",
        "        for j in range(1, k):\n",
        "            s = softmax(theta[j*X.shape[1]:(j+1)*X.shape[1]], X[i])\n",
        "            hess[j*X.shape[1]:(j+1)*X.shape[1], j*X.shape[1]:(j+1)*X.shape[1]] += np.outer(s, s)\n",
        "    for j in range(k):\n",
        "        hess[j*X.shape[1]:(j+1)*X.shape[1], j*X.shape[1]:(j+1)*X.shape[1]] -= np.diag(softmax(theta[j*X.shape[1]:(j+1)*X.shape[1]], X).flatten())\n",
        "    return hess / n\n",
        "\n",
        "def newton_raphson(X, y, max_iters=100, tol=1e-6):\n",
        "    \"\"\"Newton-Raphson method for softmax regression.\"\"\"\n",
        "    n = X.shape[0]\n",
        "    k = y.shape[1]\n",
        "    theta = np.zeros(X.shape[1] * k)\n",
        "    for i in range(max_iters):\n",
        "        J = neg_log_likelihood(theta, X, y)\n",
        "        grad = gradient(theta, X, y)\n",
        "        hess = hessian(theta, X, y)\n",
        "        theta_new = theta - np.linalg.inv(hess).dot(grad)\n",
        "        if np.abs(J - neg_log_likelihood(theta_new, X, y)) < tol:\n",
        "            return theta_new\n",
        "        theta = theta_new\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "ZFH6uSQRRIrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y_onehot = np.zeros((y.size, y.max() + 1))\n",
        "y_onehot[np.arange(y.size), y] = 1\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=0)\n",
        "theta = newton_raphson(X_train, y_train)\n",
        "y_pred = np.argmax(np.dot(X_test, theta.reshape((-1, X_train.shape[1])).T), axis=1)\n",
        "accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "_iatos-qC7Ds",
        "outputId": "fd14fa57-93ef-4837-d811-aa8f0b0fddef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-61c7e25a1dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton_raphson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b6bdccb089a5>\u001b[0m in \u001b[0;36mnewton_raphson\u001b[0;34m(X, y, max_iters, tol)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mhess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mtheta_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b6bdccb089a5>\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(theta, X, y)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mhess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mhess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhess\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b6bdccb089a5>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(theta, x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Softmax function for a sample x with weight vector theta.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (4,) and (120,4) not aligned: 4 (dim 0) != 120 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(X, theta):\n",
        "    \"\"\"Compute the softmax of the scores for each example in X.\"\"\"\n",
        "    scores = np.dot(X, theta)\n",
        "    exp_scores = np.exp(scores)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "def newton_raphson(X, y):\n",
        "    \"\"\"Compute the optimal weight vector for the softmax function using Newton-Raphson.\"\"\"\n",
        "    num_features = X.shape[1]\n",
        "    num_classes = y.shape[1]\n",
        "    theta = np.zeros(num_features * num_classes)\n",
        "    for i in range(10):\n",
        "        # Compute the softmax probabilities\n",
        "        probs = softmax(X, theta)\n",
        "\n",
        "        # Compute the gradient of the log-likelihood\n",
        "        error = y - probs\n",
        "        grad = -np.dot(X.T, error.flatten())\n",
        "\n",
        "        # Compute the Hessian of the log-likelihood\n",
        "        hess = np.zeros((num_features * num_classes, num_features * num_classes))\n",
        "        for j in range(num_classes):\n",
        "            hess_j = np.dot(X.T, (probs[:, j] * (1 - probs[:, j]))[:, None] * X)\n",
        "            hess[j*num_features:(j+1)*num_features, j*num_features:(j+1)*num_features] = hess_j\n",
        "        hess_inv = np.linalg.inv(hess)\n",
        "\n",
        "        # Update the weight vector\n",
        "        delta = np.dot(hess_inv, grad)\n",
        "        theta -= delta\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Simulate data with 3 classes and 4 features\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(120, 4)\n",
        "y = np.random.randint(0, 3, size=120)\n",
        "y_onehot = np.zeros((y.size, y.max() + 1))\n",
        "y_onehot[np.arange(y.size), y] = 1\n",
        "\n",
        "# Run the Newton-Raphson algorithm on the simulated data\n",
        "theta = newton_raphson(X, y_onehot)\n",
        "\n",
        "# Compute the predicted class probabilities for the first 10 examples\n",
        "probs = softmax(X[:10, :], theta)\n",
        "print(probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "aJl9YY_-DfNA",
        "outputId": "ae90444c-dfac-42f0-d9fc-e7ec1e71aeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e08b157f9683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Run the Newton-Raphson algorithm on the simulated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewton_raphson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Compute the predicted class probabilities for the first 10 examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e08b157f9683>\u001b[0m in \u001b[0;36mnewton_raphson\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Compute the softmax probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Compute the gradient of the log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e08b157f9683>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(X, theta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Compute the softmax of the scores for each example in X.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mexp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mexp_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (120,4) and (12,) not aligned: 4 (dim 1) != 12 (dim 0)"
          ]
        }
      ]
    }
  ]
}